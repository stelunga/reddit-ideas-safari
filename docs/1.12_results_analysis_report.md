# Reddit Safari: Algorithm Analysis & Improvement Options

## TL;DR

**Your intuition may be right.** The semantic-only approach finds more potentially valid opportunities but includes noise. The LLM+semantic hybrid has better precision but likely **lower recall** and is significantly slower. Neither approach is optimal—there's room for a middle ground.

---

## Comparing the Two Approaches

### Semantic Search Only (dated reports)

| Metric | Beekeeping | Urban Planning | Specialty Crops |
|--------|------------|----------------|-----------------|
| **Candidates Found** | 20+ | 18 | 10 |
| **True Positives** | ~8-10 | ~6-8 | ~4 |
| **Clear Noise** | ~8-10 | ~5-6 | ~4 |
| **Borderline** | ~3-5 | ~4-5 | ~2 |

**Observed Issues:**
- ✅ **Finds good opportunities** (e.g., "What software do you use?", "Any beekeeping application users?", "How do you keep records?")
- ❌ **Duplicates not deduplicated** (same post appearing 2-3 times)
- ❌ **Struggle keywords overbroad**: "pain", "hate" match literal meanings (physical pain, hate my neighbor)
- ❌ **Irrelevant posts with high scores**: "Beekeeping is way more about patience" (0.53 score) — not a software opportunity
- ❌ **Students/hobbyists mixed with professionals**

### LLM + Semantic Hybrid (verified reports)

| Metric | Beekeeping | Urban Planning | Specialty Crops |
|--------|------------|----------------|-----------------|
| **Candidates Found** | 9 | 7 | 1 |
| **Estimated Precision** | ~90% | ~85% | ~100% |

**Observed Issues:**
- ✅ **Much higher precision** — almost all results are genuine opportunities
- ❌ **Dramatically lower volume** — specialty crops went from 10 → 1
- ❌ **LLM "hallucinates" reasoning** — e.g., "Short Beekeeper Problem" (about being physically short) is accepted with reasoning about "paper and pencil"
- ❌ **Slow** — each LLM call adds ~1-3s latency
- ❌ **Inconsistent** — LLM may accept/reject similar posts differently

---

## Root Causes of Issues

### 1. **Binary Anchor Semantic Scoring is Too Simple**

Your current approach uses 2 hardcoded anchors:
```python
opp_text = "I hate this software. I use spreadsheets..."
noise_text = "I hate my boss. My salary is too low..."
```

**Problem:** This is essentially asking "which is this post closer to?" but:
- Many valid posts don't match either template
- Distance-based scoring doesn't capture **why** something is an opportunity
- threshold of 0.30 is too permissive, letting noise through

### 2. **Keyword Matching is Context-Blind**

"hate", "pain", "frustrating" match too broadly without context:
- "I hate mites damaging my hives" → valid frustration
- "I hate my job" → career complaint (noise)
- "Revolutionary Beekeeping Technique... Say Goodbye to Back Pain" → clickbait

### 3. **LLM as Final Judge is Overkill and Unreliable**

Using Llama 3 for every candidate is:
- **Slow** (each call = 1-3s)
- **Inconsistent** (temperature/sampling variance)
- **Too late in pipeline** (you've already done expensive scraping)

### 4. **Post Body Truncation Loses Context**

```python
score = calculate_semantic_score(f"{data['title']} {data['body'][:300]}", industry)
```
300 chars often cuts off the most valuable part (the details of the problem).

---

## Improvement Options (Low → High Effort)

### Option 1: Quick Wins (Low Effort)
Improvements to current approach without architectural changes.

| Change | Impact | Effort |
|--------|--------|--------|
| **Deduplicate by URL immediately** | Fixes duplicate issue | 5 min |
| **Expand body to 600-800 chars for scoring** | Better semantic signal | 5 min |
| **Raise semantic threshold to 0.40-0.45** | Reduces noise | 5 min |
| **Add contextual keyword matching** | "hate" + "software" ≠ "hate" alone | 30 min |
| **Add blacklist for title patterns** | "Rookie here", "Student", "Looking to start" | 15 min |

### Option 2: Better Semantic Scoring (Medium Effort)
Replace binary anchor with **multiple pain pattern exemplars**.

```python
# Instead of 2 anchors, use 5-10 gold examples
gold_opportunities = [
    "We use Excel to track all our hive inspections. It's terrible.",
    "Is there any software that can automate our crop traceability reporting?",
    "The software we use crashes constantly. Looking for alternatives.",
    "We still do all scheduling on paper. Takes hours every week.",
    # ... more real examples from your verified reports
]

# Score = max similarity to ANY gold example
best_score = max(cos_sim(post, example) for example in gold_opportunities)
```

**Benefits:** More nuanced matching, better generalization.
**Effort:** ~2 hours

### Option 3: Aspect-Based Pain Detection (Medium-High Effort)
Use NLP to extract **structured pain signals** instead of just keywords.

| Aspect | Signal |
|--------|--------|
| **Tool Complaint** | Mentions software/tool + negative sentiment |
| **Manual Process** | Describes repetitive task + time cost |
| **Seeking Alternative** | Question format + existing solution mentioned |
| **Cost Issue** | Price + "too expensive" / "can't afford" |

**Implementation:** 
- Use sentiment analysis on sentences, not whole posts
- Extract noun phrases being complained about
- Classify pain **type** (Tool, Process, Cost, UX)

**Effort:** ~1-2 days

### Option 4: Fine-Tuned Classifier (High Effort, Best Results)
Train a small classifier specifically for opportunity detection.

**Approach:**
1. **Label ~100-200 examples** from your existing reports (opportunity/noise)
2. **Fine-tune** a small model (DistilBERT, ~70M params) on your labels
3. **Replace** semantic scoring with classifier confidence

**Benefits:**
- Learns YOUR definition of opportunity
- Fast inference (~50ms vs ~2s for LLM)
- Consistent and deterministic
- Can achieve 85-95% accuracy with good labels

**Effort:** 1-2 days (if you have labeling infrastructure)

### Option 5: RAG-Enhanced Classification (High Effort, Most Flexible)
Use retrieval to find similar past-classified posts.

**Approach:**
1. **Build vector store** of all previously verified opportunities
2. **For each new post**, retrieve top-3 similar verified posts
3. **Prompt LLM** with: "Here are 3 similar posts we classified as opportunities. Is this new post also an opportunity?"

**Benefits:**
- Few-shot learning improves over time
- More interpretable (shows similar examples)
- Handles new industries without retraining

**Effort:** 2-3 days

---

## Comparison Matrix

| Approach | Precision | Recall | Speed | Effort | Best For |
|----------|-----------|--------|-------|--------|----------|
| **Current (Semantic Only)** | Medium | High | Fast | Done | Volume discovery |
| **Current (+ LLM Verify)** | High | Low | Slow | Done | High precision needs |
| **Quick Wins** | Medium+ | High | Fast | Low | Immediate improvement |
| **Multi-Exemplar Semantic** | Medium+ | High | Fast | Medium | Better without training |
| **Aspect-Based Detection** | High | High | Fast | Medium-High | Structured insights |
| **Fine-Tuned Classifier** | Very High | High | Very Fast | High | Production quality |
| **RAG Classification** | Very High | High | Medium | High | Continuous learning |

---

## Recommended Path

### Immediate (This Session)
1. Fix URL deduplication
2. Add contextual keyword matching
3. Raise semantic threshold to 0.42

### Short-Term (1-2 Days)
4. Implement multi-exemplar semantic scoring using your verified results as gold examples
5. Add title pattern blacklist

### Medium-Term (If Results Still Unsatisfying)
6. Consider fine-tuned classifier OR RAG approach depending on:
   - **Fine-tuned**: If you have a consistent definition of "opportunity"
   - **RAG**: If you want the system to learn from ongoing feedback

---

## Questions for You

1. **Is precision or recall more important?** (Would you rather see 10 great leads or 50 "maybe" leads?)
2. **How important is speed?** (Is it okay if a scan takes 10 minutes if results are better?)
3. **Do you want to invest in labeling?** (Would you manually tag ~100 posts as good/bad to train a classifier?)
4. **Are there specific industries that work better/worse?** (We could tune per-industry anchors)
